{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NzIDuONodenW",
        "ka-ZRtQvjBex",
        "gGuKmH_ijf7U",
        "Hjwf2HCol3XP",
        "kop-C96Aml8O",
        "qEAzOd47mv1Z",
        "5YdPG4HXnNsh"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf"
      },
      "source": [
        "# Twin-Delayed DDPG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J"
      },
      "source": [
        "## Installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f153c0-4788-4f1f-df44-e668072d3316"
      },
      "source": [
        "!pip install pybullet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (91.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 91.7 MB 100 kB/s \n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c146d846-9d8a-43dd-9be0-18a6a9719022"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr"
      },
      "source": [
        "## Step 1: We initialize the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_memory=1e6):\n",
        "    self.memory = []\n",
        "    self.max_memory = max_memory\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    if len(self.memory) == self.max_memory:\n",
        "      self.memory[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_memory\n",
        "    else:\n",
        "      self.memory.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    random_index = np.random.randint(0, len(self.memory), size = batch_size)\n",
        "    batch_state, batch_next_state, batch_action, batch_reward, batch_done = [], [], [], [], []\n",
        "    \n",
        "    for i in random_index:\n",
        "      state, next_state, action, reward, done = self.memory[i]\n",
        "      batch_state.append(np.array(state, copy=False))\n",
        "      batch_next_state.append(np.array(next_state, copy=False))\n",
        "      batch_action.append(np.array(action, copy=False))\n",
        "      batch_reward.append(np.array(reward, copy=False))\n",
        "      batch_done.append(np.array(done, copy=False))\n",
        "\n",
        "    return np.array(batch_state), np.array(batch_next_state), np.array(batch_action), np.array(batch_reward).reshape(-1,1), np.array(batch_done).reshape(-1,1)\n"
      ],
      "metadata": {
        "id": "ChTIPEhHAfsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD"
      },
      "source": [
        "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  \"\"\"\n",
        "  since there is only one method apart from init\n",
        "  the Actor class can just initiate forward method without\n",
        "  the need of explicitly stating actor_target = Actor(state_dim, action_dim, max_action)\n",
        "  actor_target.forward(X) = actor_target(X)\n",
        "  \"\"\"\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "WZo4KlbvR4-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7"
      },
      "source": [
        "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "\n",
        "    #Forward-Propagation on the first Critic neural network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "\n",
        "    # Forward-Propagation on the second Crtic neural network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "metadata": {
        "id": "fR6xZ3A6uyxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW"
      },
      "source": [
        "## Steps 4 to 15: Training Process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1,-1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  \"\"\"\n",
        "  iteration = number of iterations of the whole training\n",
        "  discount = used for the formula of the actor and critic target discount= y, reward = r, Q = r+y*min(Qt1,Qt2)\n",
        "  tau = changes the value of the actor and critic adjustment and inserts in actor and critic target to get closer to the actor model\n",
        "  policy_noise = exploration for sampling noise \n",
        "  noise_clip = clips the noise\n",
        "  policy_freq = the delayed that allows updates once every two iteration because number is 2\n",
        "  \"\"\"\n",
        "  def train(self, replay_buffer, iterations, batch_size= 100, discount =0.99, tau=0.005, policy_noise=0.2, noise_clip = 0.5, policy_freq = 2):\n",
        "\n",
        "    for it in range(iterations):\n",
        "\n",
        "      #step 4 we sample a batch of transitions (s,s',a,r)\n",
        "      batch_state, batch_next_state, batch_action, batch_reward, batch_done = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_state).to(device)\n",
        "      next_state = torch.Tensor(batch_next_state).to(device)\n",
        "      action = torch.Tensor(batch_action).to(device)\n",
        "      reward = torch.Tensor(batch_reward).to(device)\n",
        "      done = torch.Tensor(batch_done).to(device)\n",
        "\n",
        "      #step 5: From the next state s', the Actor target plays the next action a'\n",
        "      #self.actor_target.forward(next_state) or self.actor_target(next_state)\n",
        "      next_action = self.actor_target.forward(next_state)\n",
        "      \n",
        "      # Step 6: we add Guassian noise to this next action a'. and we clamp it in a rnage of values supported by the environment\n",
        "      noise = torch.Tensor(batch_action).data.normal_(0,policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "      # Step 7: The two Critic targets take each the couple (s', a') as input and return two Q-values Qt(s', a') and Qt2(s',a') as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target.forward(next_state, next_action)\n",
        "\n",
        "      # Step 8: We keep the minimum of these two Q-values:min(Qt1, Qt2)\n",
        "      min_target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "      # Step 9: we get the final target of the two Critic models, which is: Qt = r + y *min(Qt1, Qt2), where y is the discounted factor\n",
        "      target_Q = reward + ((1-done) * (discount * min_target_Q)).detach()\n",
        "\n",
        "      # Step 10: The Two Critic models take each the couple (s,a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic.forward(state, action)\n",
        "\n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a),Qt) + MSE_Loss(Q2(s,a),Qt)\n",
        "      \"\"\"critic_loss = torch.L1loss()\n",
        "      loss_Q1 = critic_loss(current_Q1, target_Q)\n",
        "      loss_Q2 = critic_loss(current_Q2, target_Q)\n",
        "      loss_Q = loss_Q1 + loss_Q2\n",
        "      loss_Q = loss_Q.backward()\"\"\"\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "      # Step 12: we backpropagate this critic loss and update the parameters of the two critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "\n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1-tau) * target_param.data)\n",
        "\n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "metadata": {
        "id": "ptM25ZRQkDhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-ZRtQvjBex"
      },
      "source": [
        "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM"
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U"
      },
      "source": [
        "## We set the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk"
      },
      "source": [
        "env_name = \"HalfCheetahBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_iteration = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_iteration = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f99e8836-69aa-4a90-91ad-d34014d6b576"
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kop-C96Aml8O"
      },
      "source": [
        "## We create a folder inside which will be saved the trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb"
      },
      "source": [
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z"
      },
      "source": [
        "## We create the PyBullet environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ee40d4-d7bd-4ff1-ec87-2d83fd60d953"
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YdPG4HXnNsh"
      },
      "source": [
        "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200d7f3e-f86b-4aec-b0b3-cf55f323d5dc"
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.8751645  -0.60804224 -0.5540787  -0.06032826  0.23912275 -0.8904909 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem"
      },
      "source": [
        "## We create the policy network (the Actor model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTVvG7F8_EWg"
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI60VN2Unklh"
      },
      "source": [
        "## We create the Experience Replay memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd-ZsdXR_LgV"
      },
      "source": [
        "replay_buffer = ReplayBuffer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYOpCyiDnw7s"
      },
      "source": [
        "## We define a list where all the evaluation results over 10 episodes are stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16643ed8-d3a1-4197-f3cd-b70edaf6ccad"
      },
      "source": [
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1433.638437\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-4b3p6rglE"
      },
      "source": [
        "## We create a new folder directory in which the final results (videos of the agent) will be populated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03"
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm"
      },
      "source": [
        "## We initialize the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT"
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b60ea12-330a-45c1-acf1-7c3e643e36cd"
      },
      "source": [
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_iteration:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # If we are not at the very beginning, we start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # We evaluate the episode and we save the policy\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      evaluations.append(evaluate_policy(policy))\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # When the training step is done, we reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_iteration:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # We check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # We increase the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: -1260.9730840132665\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: -1408.2003652503483\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: -1499.8663087970695\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: -1328.0979067715618\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: -1129.7958805905603\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1607.418182\n",
            "---------------------------------------\n",
            "Total Timesteps: 6000 Episode Num: 6 Reward: -1379.8568983240293\n",
            "Total Timesteps: 7000 Episode Num: 7 Reward: -1282.2477017951865\n",
            "Total Timesteps: 8000 Episode Num: 8 Reward: -1309.3805990166695\n",
            "Total Timesteps: 9000 Episode Num: 9 Reward: -1221.3233994468571\n",
            "Total Timesteps: 10000 Episode Num: 10 Reward: -1282.0660299469012\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1755.365854\n",
            "---------------------------------------\n",
            "Total Timesteps: 11000 Episode Num: 11 Reward: -1755.7265493230507\n",
            "Total Timesteps: 12000 Episode Num: 12 Reward: -1156.0695966457804\n",
            "Total Timesteps: 13000 Episode Num: 13 Reward: 292.7980289199731\n",
            "Total Timesteps: 14000 Episode Num: 14 Reward: 290.3506139736147\n",
            "Total Timesteps: 15000 Episode Num: 15 Reward: -239.23352708811515\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1626.536905\n",
            "---------------------------------------\n",
            "Total Timesteps: 16000 Episode Num: 16 Reward: -1649.0726650790918\n",
            "Total Timesteps: 17000 Episode Num: 17 Reward: -1484.8388007495628\n",
            "Total Timesteps: 18000 Episode Num: 18 Reward: 297.10844604224496\n",
            "Total Timesteps: 19000 Episode Num: 19 Reward: -266.4891256124144\n",
            "Total Timesteps: 20000 Episode Num: 20 Reward: 464.9885961768658\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 384.088092\n",
            "---------------------------------------\n",
            "Total Timesteps: 21000 Episode Num: 21 Reward: 370.0213907270646\n",
            "Total Timesteps: 22000 Episode Num: 22 Reward: 392.1918364952237\n",
            "Total Timesteps: 23000 Episode Num: 23 Reward: 294.4250873485808\n",
            "Total Timesteps: 24000 Episode Num: 24 Reward: 324.961921735636\n",
            "Total Timesteps: 25000 Episode Num: 25 Reward: 439.1310872330537\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 281.453578\n",
            "---------------------------------------\n",
            "Total Timesteps: 26000 Episode Num: 26 Reward: 123.31040711005241\n",
            "Total Timesteps: 27000 Episode Num: 27 Reward: 315.0080551189414\n",
            "Total Timesteps: 28000 Episode Num: 28 Reward: 256.32688503050656\n",
            "Total Timesteps: 29000 Episode Num: 29 Reward: 427.6145891031679\n",
            "Total Timesteps: 30000 Episode Num: 30 Reward: -1487.989951343219\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 556.290926\n",
            "---------------------------------------\n",
            "Total Timesteps: 31000 Episode Num: 31 Reward: 531.1923174995721\n",
            "Total Timesteps: 32000 Episode Num: 32 Reward: 547.0095935772041\n",
            "Total Timesteps: 33000 Episode Num: 33 Reward: 317.84770992823536\n",
            "Total Timesteps: 34000 Episode Num: 34 Reward: -483.13513067609296\n",
            "Total Timesteps: 35000 Episode Num: 35 Reward: 633.7801111591672\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 205.863079\n",
            "---------------------------------------\n",
            "Total Timesteps: 36000 Episode Num: 36 Reward: 217.13341061818758\n",
            "Total Timesteps: 37000 Episode Num: 37 Reward: 514.298228440052\n",
            "Total Timesteps: 38000 Episode Num: 38 Reward: 614.1750396683676\n",
            "Total Timesteps: 39000 Episode Num: 39 Reward: 544.6169419199039\n",
            "Total Timesteps: 40000 Episode Num: 40 Reward: 342.8424952838478\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 125.304017\n",
            "---------------------------------------\n",
            "Total Timesteps: 41000 Episode Num: 41 Reward: 165.0094066291054\n",
            "Total Timesteps: 42000 Episode Num: 42 Reward: 507.21420938686924\n",
            "Total Timesteps: 43000 Episode Num: 43 Reward: 479.8587614909157\n",
            "Total Timesteps: 44000 Episode Num: 44 Reward: 455.70398304727416\n",
            "Total Timesteps: 45000 Episode Num: 45 Reward: 456.9976858062726\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 547.521956\n",
            "---------------------------------------\n",
            "Total Timesteps: 46000 Episode Num: 46 Reward: 488.97022051380293\n",
            "Total Timesteps: 47000 Episode Num: 47 Reward: 224.7097721712605\n",
            "Total Timesteps: 48000 Episode Num: 48 Reward: 639.4576728228084\n",
            "Total Timesteps: 49000 Episode Num: 49 Reward: 659.3732727385164\n",
            "Total Timesteps: 50000 Episode Num: 50 Reward: 641.2162838501097\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 583.364318\n",
            "---------------------------------------\n",
            "Total Timesteps: 51000 Episode Num: 51 Reward: 498.8059096032833\n",
            "Total Timesteps: 52000 Episode Num: 52 Reward: 551.365080773322\n",
            "Total Timesteps: 53000 Episode Num: 53 Reward: 604.1355835120976\n",
            "Total Timesteps: 54000 Episode Num: 54 Reward: 610.74212300659\n",
            "Total Timesteps: 55000 Episode Num: 55 Reward: -1656.0602103032234\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 549.986456\n",
            "---------------------------------------\n",
            "Total Timesteps: 56000 Episode Num: 56 Reward: 494.3823064284062\n",
            "Total Timesteps: 57000 Episode Num: 57 Reward: -957.1968628606372\n",
            "Total Timesteps: 58000 Episode Num: 58 Reward: -1604.1255376603644\n",
            "Total Timesteps: 59000 Episode Num: 59 Reward: -1665.699670269128\n",
            "Total Timesteps: 60000 Episode Num: 60 Reward: -1634.0508717085877\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1721.794469\n",
            "---------------------------------------\n",
            "Total Timesteps: 61000 Episode Num: 61 Reward: -1724.6154418775982\n",
            "Total Timesteps: 62000 Episode Num: 62 Reward: -1663.2125138744248\n",
            "Total Timesteps: 63000 Episode Num: 63 Reward: -565.0549168324515\n",
            "Total Timesteps: 64000 Episode Num: 64 Reward: -281.65862711919425\n",
            "Total Timesteps: 65000 Episode Num: 65 Reward: 588.2007532203877\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 559.937746\n",
            "---------------------------------------\n",
            "Total Timesteps: 66000 Episode Num: 66 Reward: 665.5467039338054\n",
            "Total Timesteps: 67000 Episode Num: 67 Reward: 570.6103720953503\n",
            "Total Timesteps: 68000 Episode Num: 68 Reward: 653.5970312788506\n",
            "Total Timesteps: 69000 Episode Num: 69 Reward: 544.9021543093148\n",
            "Total Timesteps: 70000 Episode Num: 70 Reward: -665.4944660172026\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1382.559969\n",
            "---------------------------------------\n",
            "Total Timesteps: 71000 Episode Num: 71 Reward: -1649.545866698065\n",
            "Total Timesteps: 72000 Episode Num: 72 Reward: -1621.178865641435\n",
            "Total Timesteps: 73000 Episode Num: 73 Reward: 569.0079008869232\n",
            "Total Timesteps: 74000 Episode Num: 74 Reward: -878.4307302123949\n",
            "Total Timesteps: 75000 Episode Num: 75 Reward: 465.6768340323814\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1130.108009\n",
            "---------------------------------------\n",
            "Total Timesteps: 76000 Episode Num: 76 Reward: -822.9777540179955\n",
            "Total Timesteps: 77000 Episode Num: 77 Reward: -1474.431920939328\n",
            "Total Timesteps: 78000 Episode Num: 78 Reward: -1294.379930419497\n",
            "Total Timesteps: 79000 Episode Num: 79 Reward: -1628.250009114973\n",
            "Total Timesteps: 80000 Episode Num: 80 Reward: 647.6227794986793\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 718.709748\n",
            "---------------------------------------\n",
            "Total Timesteps: 81000 Episode Num: 81 Reward: 540.822945286238\n",
            "Total Timesteps: 82000 Episode Num: 82 Reward: 616.9480260019913\n",
            "Total Timesteps: 83000 Episode Num: 83 Reward: 537.1541268220055\n",
            "Total Timesteps: 84000 Episode Num: 84 Reward: -222.52515993339637\n",
            "Total Timesteps: 85000 Episode Num: 85 Reward: 260.2478192678044\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 631.231577\n",
            "---------------------------------------\n",
            "Total Timesteps: 86000 Episode Num: 86 Reward: 632.0341758505745\n",
            "Total Timesteps: 87000 Episode Num: 87 Reward: 649.0826427050382\n",
            "Total Timesteps: 88000 Episode Num: 88 Reward: 259.7566789893856\n",
            "Total Timesteps: 89000 Episode Num: 89 Reward: 574.1078629511788\n",
            "Total Timesteps: 90000 Episode Num: 90 Reward: -191.44979909485528\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 584.349787\n",
            "---------------------------------------\n",
            "Total Timesteps: 91000 Episode Num: 91 Reward: 561.7393892231753\n",
            "Total Timesteps: 92000 Episode Num: 92 Reward: 547.8434918971465\n",
            "Total Timesteps: 93000 Episode Num: 93 Reward: 524.8091167693327\n",
            "Total Timesteps: 94000 Episode Num: 94 Reward: 642.3313426876394\n",
            "Total Timesteps: 95000 Episode Num: 95 Reward: 523.8721468863722\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 626.210437\n",
            "---------------------------------------\n",
            "Total Timesteps: 96000 Episode Num: 96 Reward: 582.2959128169217\n",
            "Total Timesteps: 97000 Episode Num: 97 Reward: 936.3665005995169\n",
            "Total Timesteps: 98000 Episode Num: 98 Reward: 959.2959444836952\n",
            "Total Timesteps: 99000 Episode Num: 99 Reward: 1027.5905242397594\n",
            "Total Timesteps: 100000 Episode Num: 100 Reward: 1034.9721599158113\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 860.372594\n",
            "---------------------------------------\n",
            "Total Timesteps: 101000 Episode Num: 101 Reward: 851.2736441258055\n",
            "Total Timesteps: 102000 Episode Num: 102 Reward: 658.1415105706462\n",
            "Total Timesteps: 103000 Episode Num: 103 Reward: 744.6338925099375\n",
            "Total Timesteps: 104000 Episode Num: 104 Reward: 786.9766380663298\n",
            "Total Timesteps: 105000 Episode Num: 105 Reward: 764.1824718593678\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 796.209403\n",
            "---------------------------------------\n",
            "Total Timesteps: 106000 Episode Num: 106 Reward: 819.4633681675385\n",
            "Total Timesteps: 107000 Episode Num: 107 Reward: 907.1285682167857\n",
            "Total Timesteps: 108000 Episode Num: 108 Reward: 1016.0875204147595\n",
            "Total Timesteps: 109000 Episode Num: 109 Reward: 1124.7239914325478\n",
            "Total Timesteps: 110000 Episode Num: 110 Reward: 1148.3242319623275\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 677.631626\n",
            "---------------------------------------\n",
            "Total Timesteps: 111000 Episode Num: 111 Reward: 1048.5675219569077\n",
            "Total Timesteps: 112000 Episode Num: 112 Reward: 1325.0154084736857\n",
            "Total Timesteps: 113000 Episode Num: 113 Reward: 988.1067959238026\n",
            "Total Timesteps: 114000 Episode Num: 114 Reward: 449.3779945713192\n",
            "Total Timesteps: 115000 Episode Num: 115 Reward: 1212.8021723070688\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -1141.111331\n",
            "---------------------------------------\n",
            "Total Timesteps: 116000 Episode Num: 116 Reward: -1612.249881063676\n",
            "Total Timesteps: 117000 Episode Num: 117 Reward: 1225.2988454071597\n",
            "Total Timesteps: 118000 Episode Num: 118 Reward: 1240.5428815893351\n",
            "Total Timesteps: 119000 Episode Num: 119 Reward: 1334.9388405028471\n",
            "Total Timesteps: 120000 Episode Num: 120 Reward: 882.8962912652756\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 522.553044\n",
            "---------------------------------------\n",
            "Total Timesteps: 121000 Episode Num: 121 Reward: 1245.2680773137374\n",
            "Total Timesteps: 122000 Episode Num: 122 Reward: 587.8368137845606\n",
            "Total Timesteps: 123000 Episode Num: 123 Reward: 1474.3650512339646\n",
            "Total Timesteps: 124000 Episode Num: 124 Reward: 229.10889714562217\n",
            "Total Timesteps: 125000 Episode Num: 125 Reward: 451.9903160667614\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -391.809329\n",
            "---------------------------------------\n",
            "Total Timesteps: 126000 Episode Num: 126 Reward: -252.7714351053805\n",
            "Total Timesteps: 127000 Episode Num: 127 Reward: 575.5198344831105\n",
            "Total Timesteps: 128000 Episode Num: 128 Reward: 1417.61250729904\n",
            "Total Timesteps: 129000 Episode Num: 129 Reward: 656.9681402149138\n",
            "Total Timesteps: 130000 Episode Num: 130 Reward: -1388.1137263328899\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: -86.784981\n",
            "---------------------------------------\n",
            "Total Timesteps: 131000 Episode Num: 131 Reward: -1223.1188935100706\n",
            "Total Timesteps: 132000 Episode Num: 132 Reward: 531.5802039609575\n",
            "Total Timesteps: 133000 Episode Num: 133 Reward: 1535.530243122877\n",
            "Total Timesteps: 134000 Episode Num: 134 Reward: 1402.7963092361465\n",
            "Total Timesteps: 135000 Episode Num: 135 Reward: 1333.9715050287218\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1426.660450\n",
            "---------------------------------------\n",
            "Total Timesteps: 136000 Episode Num: 136 Reward: 1530.9657939860708\n",
            "Total Timesteps: 137000 Episode Num: 137 Reward: 1419.0855683728919\n",
            "Total Timesteps: 138000 Episode Num: 138 Reward: 1502.5234761142735\n",
            "Total Timesteps: 139000 Episode Num: 139 Reward: 1469.4351506543399\n",
            "Total Timesteps: 140000 Episode Num: 140 Reward: 1493.1131267668939\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1483.458922\n",
            "---------------------------------------\n",
            "Total Timesteps: 141000 Episode Num: 141 Reward: 1434.5386074759876\n",
            "Total Timesteps: 142000 Episode Num: 142 Reward: 1488.3326792299126\n",
            "Total Timesteps: 143000 Episode Num: 143 Reward: 1449.3004936586124\n",
            "Total Timesteps: 144000 Episode Num: 144 Reward: 1600.6458043927546\n",
            "Total Timesteps: 145000 Episode Num: 145 Reward: 1565.3054054712998\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1626.527972\n",
            "---------------------------------------\n",
            "Total Timesteps: 146000 Episode Num: 146 Reward: 1551.5650032486296\n",
            "Total Timesteps: 147000 Episode Num: 147 Reward: 1650.9568868142555\n",
            "Total Timesteps: 148000 Episode Num: 148 Reward: 1604.1678888413223\n",
            "Total Timesteps: 149000 Episode Num: 149 Reward: 1625.155666139799\n",
            "Total Timesteps: 150000 Episode Num: 150 Reward: 1620.9007772709433\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1716.871643\n",
            "---------------------------------------\n",
            "Total Timesteps: 151000 Episode Num: 151 Reward: 1713.444993863174\n",
            "Total Timesteps: 152000 Episode Num: 152 Reward: 1573.120808006308\n",
            "Total Timesteps: 153000 Episode Num: 153 Reward: 1734.101275747838\n",
            "Total Timesteps: 154000 Episode Num: 154 Reward: 1634.1038038812358\n",
            "Total Timesteps: 155000 Episode Num: 155 Reward: 1663.2601175325656\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1727.670081\n",
            "---------------------------------------\n",
            "Total Timesteps: 156000 Episode Num: 156 Reward: 1727.7278804507375\n",
            "Total Timesteps: 157000 Episode Num: 157 Reward: 1692.5559935018161\n",
            "Total Timesteps: 158000 Episode Num: 158 Reward: 1689.0110910148362\n",
            "Total Timesteps: 159000 Episode Num: 159 Reward: 1750.0420329330211\n",
            "Total Timesteps: 160000 Episode Num: 160 Reward: 1768.5672442467503\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1750.022363\n",
            "---------------------------------------\n",
            "Total Timesteps: 161000 Episode Num: 161 Reward: 1788.487891527876\n",
            "Total Timesteps: 162000 Episode Num: 162 Reward: 1744.1194029917185\n",
            "Total Timesteps: 163000 Episode Num: 163 Reward: 1760.0111105982626\n",
            "Total Timesteps: 164000 Episode Num: 164 Reward: 1745.0114072940453\n",
            "Total Timesteps: 165000 Episode Num: 165 Reward: 1781.874429411474\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1827.974920\n",
            "---------------------------------------\n",
            "Total Timesteps: 166000 Episode Num: 166 Reward: 1783.912096445746\n",
            "Total Timesteps: 167000 Episode Num: 167 Reward: 1716.9880686242577\n",
            "Total Timesteps: 168000 Episode Num: 168 Reward: 1797.0253673182392\n",
            "Total Timesteps: 169000 Episode Num: 169 Reward: 1740.6692397686115\n",
            "Total Timesteps: 170000 Episode Num: 170 Reward: 1608.9947144757714\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1752.502328\n",
            "---------------------------------------\n",
            "Total Timesteps: 171000 Episode Num: 171 Reward: 1724.5624007174654\n",
            "Total Timesteps: 172000 Episode Num: 172 Reward: 1760.89447839638\n",
            "Total Timesteps: 173000 Episode Num: 173 Reward: 1746.7228426400018\n",
            "Total Timesteps: 174000 Episode Num: 174 Reward: 1773.4879545288175\n",
            "Total Timesteps: 175000 Episode Num: 175 Reward: 1819.0741446871261\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1830.268920\n",
            "---------------------------------------\n",
            "Total Timesteps: 176000 Episode Num: 176 Reward: 1827.5864775211096\n",
            "Total Timesteps: 177000 Episode Num: 177 Reward: 1754.0274483200715\n",
            "Total Timesteps: 178000 Episode Num: 178 Reward: 1808.4638675561055\n",
            "Total Timesteps: 179000 Episode Num: 179 Reward: 1792.1502510024097\n",
            "Total Timesteps: 180000 Episode Num: 180 Reward: 1874.655495034245\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1746.191255\n",
            "---------------------------------------\n",
            "Total Timesteps: 181000 Episode Num: 181 Reward: 1709.2022754709558\n",
            "Total Timesteps: 182000 Episode Num: 182 Reward: 1803.5590673694473\n",
            "Total Timesteps: 183000 Episode Num: 183 Reward: 1884.7205917414392\n",
            "Total Timesteps: 184000 Episode Num: 184 Reward: 1871.7320699307058\n",
            "Total Timesteps: 185000 Episode Num: 185 Reward: 1875.2648823767995\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1866.591898\n",
            "---------------------------------------\n",
            "Total Timesteps: 186000 Episode Num: 186 Reward: 1851.3672753008705\n",
            "Total Timesteps: 187000 Episode Num: 187 Reward: 1897.3593585404128\n",
            "Total Timesteps: 188000 Episode Num: 188 Reward: 1815.1720350181986\n",
            "Total Timesteps: 189000 Episode Num: 189 Reward: 1801.747466627023\n",
            "Total Timesteps: 190000 Episode Num: 190 Reward: 1884.878679241643\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1916.278541\n",
            "---------------------------------------\n",
            "Total Timesteps: 191000 Episode Num: 191 Reward: 1863.1921006002892\n",
            "Total Timesteps: 192000 Episode Num: 192 Reward: 1889.6758709767885\n",
            "Total Timesteps: 193000 Episode Num: 193 Reward: 1864.8705800787063\n",
            "Total Timesteps: 194000 Episode Num: 194 Reward: 1889.3208142230926\n",
            "Total Timesteps: 195000 Episode Num: 195 Reward: 1907.2130569117423\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1919.745808\n",
            "---------------------------------------\n",
            "Total Timesteps: 196000 Episode Num: 196 Reward: 1876.1981098134206\n",
            "Total Timesteps: 197000 Episode Num: 197 Reward: 1871.7994777268384\n",
            "Total Timesteps: 198000 Episode Num: 198 Reward: 1879.4813906699849\n",
            "Total Timesteps: 199000 Episode Num: 199 Reward: 1831.5446473339862\n",
            "Total Timesteps: 200000 Episode Num: 200 Reward: 1840.6996905328222\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1914.281994\n",
            "---------------------------------------\n",
            "Total Timesteps: 201000 Episode Num: 201 Reward: 1877.9300656936175\n",
            "Total Timesteps: 202000 Episode Num: 202 Reward: 1889.5836233765501\n",
            "Total Timesteps: 203000 Episode Num: 203 Reward: 1895.1860100289168\n",
            "Total Timesteps: 204000 Episode Num: 204 Reward: 1909.8221378424155\n",
            "Total Timesteps: 205000 Episode Num: 205 Reward: 1877.7336857482571\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1920.070584\n",
            "---------------------------------------\n",
            "Total Timesteps: 206000 Episode Num: 206 Reward: 1843.849147597184\n",
            "Total Timesteps: 207000 Episode Num: 207 Reward: 1909.8976339743833\n",
            "Total Timesteps: 208000 Episode Num: 208 Reward: 1910.8397993807876\n",
            "Total Timesteps: 209000 Episode Num: 209 Reward: 1927.8030625791935\n",
            "Total Timesteps: 210000 Episode Num: 210 Reward: 1938.138782024185\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1909.302984\n",
            "---------------------------------------\n",
            "Total Timesteps: 211000 Episode Num: 211 Reward: 1883.7365998059227\n",
            "Total Timesteps: 212000 Episode Num: 212 Reward: 1922.9511334376173\n",
            "Total Timesteps: 213000 Episode Num: 213 Reward: 1854.7079132480344\n",
            "Total Timesteps: 214000 Episode Num: 214 Reward: 1907.6260985967435\n",
            "Total Timesteps: 215000 Episode Num: 215 Reward: 1958.9322434284024\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1988.448839\n",
            "---------------------------------------\n",
            "Total Timesteps: 216000 Episode Num: 216 Reward: 1899.3501790371724\n",
            "Total Timesteps: 217000 Episode Num: 217 Reward: 1904.022405711454\n",
            "Total Timesteps: 218000 Episode Num: 218 Reward: 1923.1840498804766\n",
            "Total Timesteps: 219000 Episode Num: 219 Reward: 1896.2865364180448\n",
            "Total Timesteps: 220000 Episode Num: 220 Reward: 1941.2571260401937\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2009.588554\n",
            "---------------------------------------\n",
            "Total Timesteps: 221000 Episode Num: 221 Reward: 1982.037463180674\n",
            "Total Timesteps: 222000 Episode Num: 222 Reward: 1934.6738290466317\n",
            "Total Timesteps: 223000 Episode Num: 223 Reward: 1947.6550845195998\n",
            "Total Timesteps: 224000 Episode Num: 224 Reward: 1926.8271110345897\n",
            "Total Timesteps: 225000 Episode Num: 225 Reward: 1916.2450659492083\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2009.885418\n",
            "---------------------------------------\n",
            "Total Timesteps: 226000 Episode Num: 226 Reward: 1951.1918298317269\n",
            "Total Timesteps: 227000 Episode Num: 227 Reward: 1900.3877382372764\n",
            "Total Timesteps: 228000 Episode Num: 228 Reward: 1806.3888809135904\n",
            "Total Timesteps: 229000 Episode Num: 229 Reward: 1715.0029450517497\n",
            "Total Timesteps: 230000 Episode Num: 230 Reward: 1937.027895710893\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1911.521831\n",
            "---------------------------------------\n",
            "Total Timesteps: 231000 Episode Num: 231 Reward: 1888.8868354165227\n",
            "Total Timesteps: 232000 Episode Num: 232 Reward: 1962.4328917357566\n",
            "Total Timesteps: 233000 Episode Num: 233 Reward: 1962.1018282708244\n",
            "Total Timesteps: 234000 Episode Num: 234 Reward: 1915.8213006512542\n",
            "Total Timesteps: 235000 Episode Num: 235 Reward: 1963.9694399859795\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2015.303751\n",
            "---------------------------------------\n",
            "Total Timesteps: 236000 Episode Num: 236 Reward: 1959.976407247674\n",
            "Total Timesteps: 237000 Episode Num: 237 Reward: 1951.44710700386\n",
            "Total Timesteps: 238000 Episode Num: 238 Reward: 1915.4645137006603\n",
            "Total Timesteps: 239000 Episode Num: 239 Reward: 1968.269266437751\n",
            "Total Timesteps: 240000 Episode Num: 240 Reward: 1972.9479976852438\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2046.326328\n",
            "---------------------------------------\n",
            "Total Timesteps: 241000 Episode Num: 241 Reward: 2020.7362659651071\n",
            "Total Timesteps: 242000 Episode Num: 242 Reward: 1986.4262636289673\n",
            "Total Timesteps: 243000 Episode Num: 243 Reward: 1994.963965896764\n",
            "Total Timesteps: 244000 Episode Num: 244 Reward: 2002.4219230468182\n",
            "Total Timesteps: 245000 Episode Num: 245 Reward: 1997.6487640136997\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2002.105719\n",
            "---------------------------------------\n",
            "Total Timesteps: 246000 Episode Num: 246 Reward: 1956.8765069111112\n",
            "Total Timesteps: 247000 Episode Num: 247 Reward: 2024.2704739343928\n",
            "Total Timesteps: 248000 Episode Num: 248 Reward: 1986.852050717944\n",
            "Total Timesteps: 249000 Episode Num: 249 Reward: 1986.096646294155\n",
            "Total Timesteps: 250000 Episode Num: 250 Reward: 1976.8432380744723\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2034.481204\n",
            "---------------------------------------\n",
            "Total Timesteps: 251000 Episode Num: 251 Reward: 1994.5833569553606\n",
            "Total Timesteps: 252000 Episode Num: 252 Reward: 1929.6178502511425\n",
            "Total Timesteps: 253000 Episode Num: 253 Reward: 2016.2756213783905\n",
            "Total Timesteps: 254000 Episode Num: 254 Reward: 1977.126655473653\n",
            "Total Timesteps: 255000 Episode Num: 255 Reward: 2000.9763352563593\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2022.130631\n",
            "---------------------------------------\n",
            "Total Timesteps: 256000 Episode Num: 256 Reward: 1991.2840741357898\n",
            "Total Timesteps: 257000 Episode Num: 257 Reward: 2002.926749366856\n",
            "Total Timesteps: 258000 Episode Num: 258 Reward: 2032.4389910500126\n",
            "Total Timesteps: 259000 Episode Num: 259 Reward: 1997.8377809117449\n",
            "Total Timesteps: 260000 Episode Num: 260 Reward: 1989.2045262420588\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2107.476780\n",
            "---------------------------------------\n",
            "Total Timesteps: 261000 Episode Num: 261 Reward: 1975.5265225435285\n",
            "Total Timesteps: 262000 Episode Num: 262 Reward: 2016.390172828914\n",
            "Total Timesteps: 263000 Episode Num: 263 Reward: 1975.8481121794207\n",
            "Total Timesteps: 264000 Episode Num: 264 Reward: 2023.1405635729132\n",
            "Total Timesteps: 265000 Episode Num: 265 Reward: 1924.426374115974\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2021.412819\n",
            "---------------------------------------\n",
            "Total Timesteps: 266000 Episode Num: 266 Reward: 1987.8911600502865\n",
            "Total Timesteps: 267000 Episode Num: 267 Reward: 2027.0697964370445\n",
            "Total Timesteps: 268000 Episode Num: 268 Reward: 2001.5769980595821\n",
            "Total Timesteps: 269000 Episode Num: 269 Reward: 1967.7545789285393\n",
            "Total Timesteps: 270000 Episode Num: 270 Reward: 1978.8995931842117\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2082.649569\n",
            "---------------------------------------\n",
            "Total Timesteps: 271000 Episode Num: 271 Reward: 2023.2253042344742\n",
            "Total Timesteps: 272000 Episode Num: 272 Reward: 2003.7959223773173\n",
            "Total Timesteps: 273000 Episode Num: 273 Reward: 1998.7409588892779\n",
            "Total Timesteps: 274000 Episode Num: 274 Reward: 2050.7475439672667\n",
            "Total Timesteps: 275000 Episode Num: 275 Reward: 1981.2455528922776\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2035.292311\n",
            "---------------------------------------\n",
            "Total Timesteps: 276000 Episode Num: 276 Reward: 1998.0380666962385\n",
            "Total Timesteps: 277000 Episode Num: 277 Reward: 2026.1688027118748\n",
            "Total Timesteps: 278000 Episode Num: 278 Reward: 2038.377894767715\n",
            "Total Timesteps: 279000 Episode Num: 279 Reward: 2028.9890018396873\n",
            "Total Timesteps: 280000 Episode Num: 280 Reward: 1961.3929922075338\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2075.665307\n",
            "---------------------------------------\n",
            "Total Timesteps: 281000 Episode Num: 281 Reward: 2039.9980639157327\n",
            "Total Timesteps: 282000 Episode Num: 282 Reward: 2030.7682495821937\n",
            "Total Timesteps: 283000 Episode Num: 283 Reward: 2040.8598857439952\n",
            "Total Timesteps: 284000 Episode Num: 284 Reward: 2017.7064160204386\n",
            "Total Timesteps: 285000 Episode Num: 285 Reward: 2001.6838499864602\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2092.872288\n",
            "---------------------------------------\n",
            "Total Timesteps: 286000 Episode Num: 286 Reward: 2036.938962326644\n",
            "Total Timesteps: 287000 Episode Num: 287 Reward: 2033.7098972000858\n",
            "Total Timesteps: 288000 Episode Num: 288 Reward: 2025.7993491442214\n",
            "Total Timesteps: 289000 Episode Num: 289 Reward: 2047.383573246035\n",
            "Total Timesteps: 290000 Episode Num: 290 Reward: 2007.861691715981\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2147.302073\n",
            "---------------------------------------\n",
            "Total Timesteps: 291000 Episode Num: 291 Reward: 2008.5859389991526\n",
            "Total Timesteps: 292000 Episode Num: 292 Reward: 2064.17244657207\n",
            "Total Timesteps: 293000 Episode Num: 293 Reward: 2022.0115825771725\n",
            "Total Timesteps: 294000 Episode Num: 294 Reward: 2028.146442379503\n",
            "Total Timesteps: 295000 Episode Num: 295 Reward: 2039.8936156487327\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2150.771492\n",
            "---------------------------------------\n",
            "Total Timesteps: 296000 Episode Num: 296 Reward: 2034.6783697160677\n",
            "Total Timesteps: 297000 Episode Num: 297 Reward: 2070.6568179995\n",
            "Total Timesteps: 298000 Episode Num: 298 Reward: 2024.5120889438683\n",
            "Total Timesteps: 299000 Episode Num: 299 Reward: 2018.9199260442908\n",
            "Total Timesteps: 300000 Episode Num: 300 Reward: 2027.2913284533\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2158.328472\n",
            "---------------------------------------\n",
            "Total Timesteps: 301000 Episode Num: 301 Reward: 2052.419037126264\n",
            "Total Timesteps: 302000 Episode Num: 302 Reward: 2069.573796585051\n",
            "Total Timesteps: 303000 Episode Num: 303 Reward: 2073.426604733769\n",
            "Total Timesteps: 304000 Episode Num: 304 Reward: 2064.431406291924\n",
            "Total Timesteps: 305000 Episode Num: 305 Reward: 2082.9687585044558\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2140.725156\n",
            "---------------------------------------\n",
            "Total Timesteps: 306000 Episode Num: 306 Reward: 2065.0928116965592\n",
            "Total Timesteps: 307000 Episode Num: 307 Reward: 2047.893724479959\n",
            "Total Timesteps: 308000 Episode Num: 308 Reward: 2057.967057517002\n",
            "Total Timesteps: 309000 Episode Num: 309 Reward: 2050.1514516170173\n",
            "Total Timesteps: 310000 Episode Num: 310 Reward: 2032.5533771187854\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2079.806762\n",
            "---------------------------------------\n",
            "Total Timesteps: 311000 Episode Num: 311 Reward: 2032.3180926079988\n",
            "Total Timesteps: 312000 Episode Num: 312 Reward: 1993.1110436311633\n",
            "Total Timesteps: 313000 Episode Num: 313 Reward: 2024.0407231215559\n",
            "Total Timesteps: 314000 Episode Num: 314 Reward: 2056.809917310485\n",
            "Total Timesteps: 315000 Episode Num: 315 Reward: 2012.8666770306293\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2112.047668\n",
            "---------------------------------------\n",
            "Total Timesteps: 316000 Episode Num: 316 Reward: 2021.5250219416673\n",
            "Total Timesteps: 317000 Episode Num: 317 Reward: 2006.603297896934\n",
            "Total Timesteps: 318000 Episode Num: 318 Reward: 2022.9462211268565\n",
            "Total Timesteps: 319000 Episode Num: 319 Reward: 2007.0511259851594\n",
            "Total Timesteps: 320000 Episode Num: 320 Reward: 2032.03908238598\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2098.893189\n",
            "---------------------------------------\n",
            "Total Timesteps: 321000 Episode Num: 321 Reward: 2010.2456333006219\n",
            "Total Timesteps: 322000 Episode Num: 322 Reward: 1996.6964011587122\n",
            "Total Timesteps: 323000 Episode Num: 323 Reward: 2040.4454225007905\n",
            "Total Timesteps: 324000 Episode Num: 324 Reward: 2038.407115663988\n",
            "Total Timesteps: 325000 Episode Num: 325 Reward: 2084.924348860812\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2152.756354\n",
            "---------------------------------------\n",
            "Total Timesteps: 326000 Episode Num: 326 Reward: 2099.3842897643895\n",
            "Total Timesteps: 327000 Episode Num: 327 Reward: 2046.9206838550426\n",
            "Total Timesteps: 328000 Episode Num: 328 Reward: 2094.0895828032235\n",
            "Total Timesteps: 329000 Episode Num: 329 Reward: 2009.048489639894\n",
            "Total Timesteps: 330000 Episode Num: 330 Reward: 2038.7815796214732\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2111.450061\n",
            "---------------------------------------\n",
            "Total Timesteps: 331000 Episode Num: 331 Reward: 2076.631561465687\n",
            "Total Timesteps: 332000 Episode Num: 332 Reward: 2056.426838207831\n",
            "Total Timesteps: 333000 Episode Num: 333 Reward: 2071.4566177096553\n",
            "Total Timesteps: 334000 Episode Num: 334 Reward: 2020.1758441360264\n",
            "Total Timesteps: 335000 Episode Num: 335 Reward: 1970.4070848458384\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2090.274025\n",
            "---------------------------------------\n",
            "Total Timesteps: 336000 Episode Num: 336 Reward: 1977.0038561867884\n",
            "Total Timesteps: 337000 Episode Num: 337 Reward: 1992.3709082594319\n",
            "Total Timesteps: 338000 Episode Num: 338 Reward: 1981.0429815876112\n",
            "Total Timesteps: 339000 Episode Num: 339 Reward: 2006.316700577722\n",
            "Total Timesteps: 340000 Episode Num: 340 Reward: 1985.1486388264836\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2101.279891\n",
            "---------------------------------------\n",
            "Total Timesteps: 341000 Episode Num: 341 Reward: 2054.9102593226808\n",
            "Total Timesteps: 342000 Episode Num: 342 Reward: 2093.6671180280355\n",
            "Total Timesteps: 343000 Episode Num: 343 Reward: 1908.8279545500598\n",
            "Total Timesteps: 344000 Episode Num: 344 Reward: 2071.0772019717297\n",
            "Total Timesteps: 345000 Episode Num: 345 Reward: 2060.98222765983\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2079.117709\n",
            "---------------------------------------\n",
            "Total Timesteps: 346000 Episode Num: 346 Reward: 2031.6168331090369\n",
            "Total Timesteps: 347000 Episode Num: 347 Reward: 2041.5447812662237\n",
            "Total Timesteps: 348000 Episode Num: 348 Reward: 2051.4436043725605\n",
            "Total Timesteps: 349000 Episode Num: 349 Reward: 2049.574852553108\n",
            "Total Timesteps: 350000 Episode Num: 350 Reward: 2026.2347816225613\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2117.480182\n",
            "---------------------------------------\n",
            "Total Timesteps: 351000 Episode Num: 351 Reward: 2048.732748834272\n",
            "Total Timesteps: 352000 Episode Num: 352 Reward: 2080.2660001691256\n",
            "Total Timesteps: 353000 Episode Num: 353 Reward: 2098.7453576857924\n",
            "Total Timesteps: 354000 Episode Num: 354 Reward: 2071.584273767437\n",
            "Total Timesteps: 355000 Episode Num: 355 Reward: 2101.567045593743\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2126.653388\n",
            "---------------------------------------\n",
            "Total Timesteps: 356000 Episode Num: 356 Reward: 2042.3449591321378\n",
            "Total Timesteps: 357000 Episode Num: 357 Reward: 2006.8055215123502\n",
            "Total Timesteps: 358000 Episode Num: 358 Reward: 2064.602265320596\n",
            "Total Timesteps: 359000 Episode Num: 359 Reward: 2038.4191186909543\n",
            "Total Timesteps: 360000 Episode Num: 360 Reward: 2080.236711809406\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2105.210199\n",
            "---------------------------------------\n",
            "Total Timesteps: 361000 Episode Num: 361 Reward: 2044.033363825676\n",
            "Total Timesteps: 362000 Episode Num: 362 Reward: 2029.0467317373036\n",
            "Total Timesteps: 363000 Episode Num: 363 Reward: 2063.801833399254\n",
            "Total Timesteps: 364000 Episode Num: 364 Reward: 2062.2509939444144\n",
            "Total Timesteps: 365000 Episode Num: 365 Reward: 1922.8739330643234\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2156.621921\n",
            "---------------------------------------\n",
            "Total Timesteps: 366000 Episode Num: 366 Reward: 2042.6569766660086\n",
            "Total Timesteps: 367000 Episode Num: 367 Reward: 2077.0156164754108\n",
            "Total Timesteps: 368000 Episode Num: 368 Reward: 2032.8350710928855\n",
            "Total Timesteps: 369000 Episode Num: 369 Reward: 2093.8857707404445\n",
            "Total Timesteps: 370000 Episode Num: 370 Reward: 2127.0081533966513\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2129.184371\n",
            "---------------------------------------\n",
            "Total Timesteps: 371000 Episode Num: 371 Reward: 1982.8462492578155\n",
            "Total Timesteps: 372000 Episode Num: 372 Reward: 2069.270540251077\n",
            "Total Timesteps: 373000 Episode Num: 373 Reward: 2050.220813229748\n",
            "Total Timesteps: 374000 Episode Num: 374 Reward: 2072.3873021220597\n",
            "Total Timesteps: 375000 Episode Num: 375 Reward: 2065.174118877153\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2185.913554\n",
            "---------------------------------------\n",
            "Total Timesteps: 376000 Episode Num: 376 Reward: 2088.6945000500905\n",
            "Total Timesteps: 377000 Episode Num: 377 Reward: 2081.3529508172833\n",
            "Total Timesteps: 378000 Episode Num: 378 Reward: 2046.7036378457997\n",
            "Total Timesteps: 379000 Episode Num: 379 Reward: 2067.5462799843876\n",
            "Total Timesteps: 380000 Episode Num: 380 Reward: 2069.1218537653704\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2143.465231\n",
            "---------------------------------------\n",
            "Total Timesteps: 381000 Episode Num: 381 Reward: 2085.315956321087\n",
            "Total Timesteps: 382000 Episode Num: 382 Reward: 2089.287223044887\n",
            "Total Timesteps: 383000 Episode Num: 383 Reward: 2043.5484871003866\n",
            "Total Timesteps: 384000 Episode Num: 384 Reward: 2102.491369519432\n",
            "Total Timesteps: 385000 Episode Num: 385 Reward: 2106.0099088622705\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2166.110640\n",
            "---------------------------------------\n",
            "Total Timesteps: 386000 Episode Num: 386 Reward: 2108.6005402686646\n",
            "Total Timesteps: 387000 Episode Num: 387 Reward: 2114.92392169069\n",
            "Total Timesteps: 388000 Episode Num: 388 Reward: 2045.0676104355125\n",
            "Total Timesteps: 389000 Episode Num: 389 Reward: 2103.981612192041\n",
            "Total Timesteps: 390000 Episode Num: 390 Reward: 2117.3485988866673\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2168.786930\n",
            "---------------------------------------\n",
            "Total Timesteps: 391000 Episode Num: 391 Reward: 2069.3264343294313\n",
            "Total Timesteps: 392000 Episode Num: 392 Reward: 2121.044860215666\n",
            "Total Timesteps: 393000 Episode Num: 393 Reward: 2063.200209713329\n",
            "Total Timesteps: 394000 Episode Num: 394 Reward: 2111.4719752918304\n",
            "Total Timesteps: 395000 Episode Num: 395 Reward: 2078.8246751937704\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2177.777527\n",
            "---------------------------------------\n",
            "Total Timesteps: 396000 Episode Num: 396 Reward: 2097.544963894947\n",
            "Total Timesteps: 397000 Episode Num: 397 Reward: 2023.6376266875786\n",
            "Total Timesteps: 398000 Episode Num: 398 Reward: 2042.3792154821087\n",
            "Total Timesteps: 399000 Episode Num: 399 Reward: 2073.87789727987\n",
            "Total Timesteps: 400000 Episode Num: 400 Reward: 1968.7430921174455\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2186.202906\n",
            "---------------------------------------\n",
            "Total Timesteps: 401000 Episode Num: 401 Reward: 2115.2167580581386\n",
            "Total Timesteps: 402000 Episode Num: 402 Reward: 2060.009378125898\n",
            "Total Timesteps: 403000 Episode Num: 403 Reward: 2079.1760316816603\n",
            "Total Timesteps: 404000 Episode Num: 404 Reward: 2095.71784418803\n",
            "Total Timesteps: 405000 Episode Num: 405 Reward: 2032.2251488977981\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2183.686696\n",
            "---------------------------------------\n",
            "Total Timesteps: 406000 Episode Num: 406 Reward: 2117.722058127127\n",
            "Total Timesteps: 407000 Episode Num: 407 Reward: 2046.7103336913285\n",
            "Total Timesteps: 408000 Episode Num: 408 Reward: 2118.5212594562595\n",
            "Total Timesteps: 409000 Episode Num: 409 Reward: 2091.7386699160643\n",
            "Total Timesteps: 410000 Episode Num: 410 Reward: 2098.5489931910283\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2165.197336\n",
            "---------------------------------------\n",
            "Total Timesteps: 411000 Episode Num: 411 Reward: 2089.443896820759\n",
            "Total Timesteps: 412000 Episode Num: 412 Reward: 2073.913004164407\n",
            "Total Timesteps: 413000 Episode Num: 413 Reward: 2111.916899418985\n",
            "Total Timesteps: 414000 Episode Num: 414 Reward: 2147.965919727959\n",
            "Total Timesteps: 415000 Episode Num: 415 Reward: 2108.1505188587157\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2169.313831\n",
            "---------------------------------------\n",
            "Total Timesteps: 416000 Episode Num: 416 Reward: 2033.8692362937384\n",
            "Total Timesteps: 417000 Episode Num: 417 Reward: 2135.310186752104\n",
            "Total Timesteps: 418000 Episode Num: 418 Reward: 2168.8979054959927\n",
            "Total Timesteps: 419000 Episode Num: 419 Reward: 2123.9700086064267\n",
            "Total Timesteps: 420000 Episode Num: 420 Reward: 2097.588998527348\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2178.760341\n",
            "---------------------------------------\n",
            "Total Timesteps: 421000 Episode Num: 421 Reward: 2135.7392578700797\n",
            "Total Timesteps: 422000 Episode Num: 422 Reward: 2152.991238921402\n",
            "Total Timesteps: 423000 Episode Num: 423 Reward: 2124.066620517079\n",
            "Total Timesteps: 424000 Episode Num: 424 Reward: 2135.473830402155\n",
            "Total Timesteps: 425000 Episode Num: 425 Reward: 2103.1819565279156\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2183.411870\n",
            "---------------------------------------\n",
            "Total Timesteps: 426000 Episode Num: 426 Reward: 2101.4119921953256\n",
            "Total Timesteps: 427000 Episode Num: 427 Reward: 2125.445024452161\n",
            "Total Timesteps: 428000 Episode Num: 428 Reward: 2109.8694373710455\n",
            "Total Timesteps: 429000 Episode Num: 429 Reward: 2087.504165613417\n",
            "Total Timesteps: 430000 Episode Num: 430 Reward: 2120.1451271428054\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2216.537155\n",
            "---------------------------------------\n",
            "Total Timesteps: 431000 Episode Num: 431 Reward: 2085.342039692141\n",
            "Total Timesteps: 432000 Episode Num: 432 Reward: 2168.0570819852073\n",
            "Total Timesteps: 433000 Episode Num: 433 Reward: 2116.4342770727744\n",
            "Total Timesteps: 434000 Episode Num: 434 Reward: 2110.662305311139\n",
            "Total Timesteps: 435000 Episode Num: 435 Reward: 2126.3264423845085\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2185.796817\n",
            "---------------------------------------\n",
            "Total Timesteps: 436000 Episode Num: 436 Reward: 2107.3354768979743\n",
            "Total Timesteps: 437000 Episode Num: 437 Reward: 2096.6907536758044\n",
            "Total Timesteps: 438000 Episode Num: 438 Reward: 2142.777221584981\n",
            "Total Timesteps: 439000 Episode Num: 439 Reward: 1982.7682933286255\n",
            "Total Timesteps: 440000 Episode Num: 440 Reward: 2127.7254605418425\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2142.732929\n",
            "---------------------------------------\n",
            "Total Timesteps: 441000 Episode Num: 441 Reward: 2047.6853726743311\n",
            "Total Timesteps: 442000 Episode Num: 442 Reward: 1980.5628586369762\n",
            "Total Timesteps: 443000 Episode Num: 443 Reward: 1961.66866937767\n",
            "Total Timesteps: 444000 Episode Num: 444 Reward: 2031.4112089239343\n",
            "Total Timesteps: 445000 Episode Num: 445 Reward: 1842.2142828493065\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 135.480610\n",
            "---------------------------------------\n",
            "Total Timesteps: 446000 Episode Num: 446 Reward: -1674.1977448492903\n",
            "Total Timesteps: 447000 Episode Num: 447 Reward: -1657.038723832934\n",
            "Total Timesteps: 448000 Episode Num: 448 Reward: -1600.1340997854397\n",
            "Total Timesteps: 449000 Episode Num: 449 Reward: 1198.3927486799823\n",
            "Total Timesteps: 450000 Episode Num: 450 Reward: -1530.0948103753753\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 281.171853\n",
            "---------------------------------------\n",
            "Total Timesteps: 451000 Episode Num: 451 Reward: 1571.6348139763938\n",
            "Total Timesteps: 452000 Episode Num: 452 Reward: 1692.1063175052197\n",
            "Total Timesteps: 453000 Episode Num: 453 Reward: 1815.9483533770822\n",
            "Total Timesteps: 454000 Episode Num: 454 Reward: 2062.6181682102806\n",
            "Total Timesteps: 455000 Episode Num: 455 Reward: 1980.6018758045977\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2192.925590\n",
            "---------------------------------------\n",
            "Total Timesteps: 456000 Episode Num: 456 Reward: 2080.6376509341603\n",
            "Total Timesteps: 457000 Episode Num: 457 Reward: 2082.1506153629734\n",
            "Total Timesteps: 458000 Episode Num: 458 Reward: 2132.092365736825\n",
            "Total Timesteps: 459000 Episode Num: 459 Reward: 2112.8814576076143\n",
            "Total Timesteps: 460000 Episode Num: 460 Reward: 2123.338340743117\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2208.134314\n",
            "---------------------------------------\n",
            "Total Timesteps: 461000 Episode Num: 461 Reward: 2126.1317345303737\n",
            "Total Timesteps: 462000 Episode Num: 462 Reward: 2149.404306383175\n",
            "Total Timesteps: 463000 Episode Num: 463 Reward: 2086.1247685896938\n",
            "Total Timesteps: 464000 Episode Num: 464 Reward: 2094.4637872151707\n",
            "Total Timesteps: 465000 Episode Num: 465 Reward: 2164.952378411273\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2216.516016\n",
            "---------------------------------------\n",
            "Total Timesteps: 466000 Episode Num: 466 Reward: 2163.8939081289254\n",
            "Total Timesteps: 467000 Episode Num: 467 Reward: 2122.960053046154\n",
            "Total Timesteps: 468000 Episode Num: 468 Reward: 2160.038300530905\n",
            "Total Timesteps: 469000 Episode Num: 469 Reward: 2102.4231062334416\n",
            "Total Timesteps: 470000 Episode Num: 470 Reward: 2130.074073709182\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2222.738376\n",
            "---------------------------------------\n",
            "Total Timesteps: 471000 Episode Num: 471 Reward: 2136.4668047279724\n",
            "Total Timesteps: 472000 Episode Num: 472 Reward: 2183.7701269721456\n",
            "Total Timesteps: 473000 Episode Num: 473 Reward: 2105.8707869550285\n",
            "Total Timesteps: 474000 Episode Num: 474 Reward: 2154.462800153518\n",
            "Total Timesteps: 475000 Episode Num: 475 Reward: 2140.3979636532417\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2232.767830\n",
            "---------------------------------------\n",
            "Total Timesteps: 476000 Episode Num: 476 Reward: 2156.3968507414756\n",
            "Total Timesteps: 477000 Episode Num: 477 Reward: 2077.7228211234196\n",
            "Total Timesteps: 478000 Episode Num: 478 Reward: 2116.7855279765536\n",
            "Total Timesteps: 479000 Episode Num: 479 Reward: 2117.6743230344573\n",
            "Total Timesteps: 480000 Episode Num: 480 Reward: 2162.210940354356\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2246.898632\n",
            "---------------------------------------\n",
            "Total Timesteps: 481000 Episode Num: 481 Reward: 2114.393177887577\n",
            "Total Timesteps: 482000 Episode Num: 482 Reward: 2135.1718823356982\n",
            "Total Timesteps: 483000 Episode Num: 483 Reward: 2115.793168454645\n",
            "Total Timesteps: 484000 Episode Num: 484 Reward: 2178.519971774878\n",
            "Total Timesteps: 485000 Episode Num: 485 Reward: 2116.0632176540535\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2157.413018\n",
            "---------------------------------------\n",
            "Total Timesteps: 486000 Episode Num: 486 Reward: 2094.2484587378985\n",
            "Total Timesteps: 487000 Episode Num: 487 Reward: 2155.702847498006\n",
            "Total Timesteps: 488000 Episode Num: 488 Reward: 2126.0616073737465\n",
            "Total Timesteps: 489000 Episode Num: 489 Reward: 2187.0084352194117\n",
            "Total Timesteps: 490000 Episode Num: 490 Reward: 2172.111439521667\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2222.573285\n",
            "---------------------------------------\n",
            "Total Timesteps: 491000 Episode Num: 491 Reward: 2142.146095644229\n",
            "Total Timesteps: 492000 Episode Num: 492 Reward: 2119.228960770157\n",
            "Total Timesteps: 493000 Episode Num: 493 Reward: 2137.8642695976814\n",
            "Total Timesteps: 494000 Episode Num: 494 Reward: 2163.608175252338\n",
            "Total Timesteps: 495000 Episode Num: 495 Reward: 2180.1754458059536\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2248.932372\n",
            "---------------------------------------\n",
            "Total Timesteps: 496000 Episode Num: 496 Reward: 2147.2579746588144\n",
            "Total Timesteps: 497000 Episode Num: 497 Reward: 2063.877141207333\n",
            "Total Timesteps: 498000 Episode Num: 498 Reward: 2164.382963618214\n",
            "Total Timesteps: 499000 Episode Num: 499 Reward: 2142.8980857379474\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2232.266526\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi6e2-_pu05e"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4d1YAMqif1"
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
        "    return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "  def forward(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2)\n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def select_action(self, state):\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "      \n",
        "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      \n",
        "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "      \n",
        "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "      if it % policy_freq == 0:\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print (\"---------------------------------------\")\n",
        "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "  print (\"---------------------------------------\")\n",
        "  return avg_reward\n",
        "\n",
        "env_name = \"HalfCheetahBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}